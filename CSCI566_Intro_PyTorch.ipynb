{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI566_Intro_PyTorch (coding).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aastha3/CSCI599-Assignment1/blob/master/CSCI566_Intro_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0tpgUDQCA2",
        "colab_type": "text"
      },
      "source": [
        "# CSCI566 Lecture 14. PyTorch and Deep Reinforcement Learning Implementation\n",
        "\n",
        "\n",
        "This notebook is for introducing basic usage of PyTorch and an example implementation of deep reinforcement learning algorithm (simple q-learning).\n",
        "\n",
        "\n",
        "It covers introduction to\n",
        "\n",
        "*   PyTorch\n",
        "*   OpenAI gym\n",
        "*   Q-learning implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_jO0H60RiSe",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "You can find a tutorial at https://bit.ly/60minblitz .  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NmH0ndrnipE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch \n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spP8Fvaaamrz",
        "colab_type": "text"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "PyTorch's tensors are similar to numpy's ndarrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6doUZOQRl7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "8fc052ba-d7de-48ea-ba34-527fd2a5cd79"
      },
      "source": [
        "# Zero tensor\n",
        "z = torch.zeros(10)\n",
        "\n",
        "print('zeros: ', z)\n",
        "print('dtype: ', z.dtype)\n",
        "print('shape: ', z.shape)\n",
        "\n",
        "# Zero tensor\n",
        "z = torch.zeros([2, 5])\n",
        "\n",
        "print('zeros: ', z)\n",
        "print('dtype: ', z.dtype)\n",
        "print('shape: ', z.shape)\n",
        "\n",
        "# Zero tensor\n",
        "z = np.zeros([2, 5])\n",
        "\n",
        "print('zeros: ', z)\n",
        "print('dtype: ', z.dtype)\n",
        "print('shape: ', z.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zeros:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "dtype:  torch.float32\n",
            "shape:  torch.Size([10])\n",
            "zeros:  tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "dtype:  torch.float32\n",
            "shape:  torch.Size([2, 5])\n",
            "zeros:  [[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "dtype:  float64\n",
            "shape:  (2, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpExm4PioQux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "981959c4-90e0-4a1d-cc07-12ba1801fa42"
      },
      "source": [
        "# One tensor\n",
        "o = torch.ones(12)\n",
        "\n",
        "print('ones: ', o)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ones:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQM8WKwoR75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf3b6451-cedf-4e19-cccc-39d526bea225"
      },
      "source": [
        "# Random tensor\n",
        "r = torch.randn(3)\n",
        "\n",
        "print('random: ', r)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random:  tensor([ 0.2532,  0.5555, -0.1350])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJOig6UoR1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "958f30f6-17bb-4c18-9de2-6573db6c4c6a"
      },
      "source": [
        "# Identity tensor\n",
        "i = torch.eye(4)\n",
        "print('identity: ', i)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "identity:  tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0TWGTokoRuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "68b9d53c-57ea-45e2-96d4-e81dd84bacb6"
      },
      "source": [
        "# Tensor from values, numpy array, list\n",
        "a_scalar = 10.10\n",
        "a = torch.tensor(a_scalar)\n",
        "\n",
        "print(a)\n",
        "\n",
        "b_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "b = torch.tensor(b_array)\n",
        "\n",
        "print(b) \n",
        "\n",
        "x_list = [5., 6., 6.]\n",
        "x = torch.tensor(x_list)\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.1000)\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([5., 6., 6.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rs6pVroRl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32f5ea07-57fa-4a5e-a05a-ddf57d1b40c9"
      },
      "source": [
        "# Tensor with autograd\n",
        "v = torch.tensor( x_list, requires_grad=True)\n",
        "\n",
        "print(v)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 6., 6.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPQl23yToRVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "58a1940a-8811-4e18-e7cf-0319c9119e5f"
      },
      "source": [
        "# Tensor to scalar\n",
        "print(a.item())\n",
        "\n",
        "\n",
        "# Tensor to numpy array\n",
        "print(x.numpy())\n",
        "\n",
        "\n",
        "#print(v.numpy()) # will generate an error\n",
        "\n",
        "# Tensor with gradient needs to be detached before numpy()\n",
        "print(v.detach().numpy())\n",
        "print(v)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.100000381469727\n",
            "[5. 6. 6.]\n",
            "[5. 6. 6.]\n",
            "tensor([5., 6., 6.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IalZ8BEygS8c",
        "colab_type": "text"
      },
      "source": [
        "### Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnZu1wcRR18o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "b89c2155-c107-4314-ac31-030a98daa76d"
      },
      "source": [
        "#print(o)\n",
        "#print(r)\n",
        "\n",
        "o = o[:3]\n",
        "\n",
        "\n",
        "# Add\n",
        "print(o + r)\n",
        "\n",
        "o = np.eye(3,3)\n",
        "\n",
        "# Element-wise multiplication\n",
        "print(r * o)\n",
        "\n",
        "# Matrix multiplication\n",
        "print(torch.matmul(r, i))\n",
        "\n",
        "# Reshape\n",
        "print(o.view(3, 2, 1)) # only change view if possible\n",
        "print(o.reshape(3, 2)) # always rearrange values in memory"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.2532, 1.5555, 0.8650])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0768a970283d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Element-wise multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Matrix multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: mul(): argument 'other' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qz2BcAAjKuU",
        "colab_type": "text"
      },
      "source": [
        "### Gradient\n",
        "The code below shows a simple example of autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDG6oP-QjMo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ab1c6709-73e7-4e96-ff9b-d6ac6d15b3d8"
      },
      "source": [
        "# y = (x - 1)^2 + 2\n",
        "\n",
        "x = torch.randn(1, requires_grad=True)\n",
        "y = (x - 1).pow(2) + 2\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.1768], requires_grad=True)\n",
            "tensor([2.0313], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mnkPh-fkQIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3083e36-58f7-46c7-ee39-d4cbfff0c8d3"
      },
      "source": [
        "y_min = np.inf\n",
        "# Find the minimum of y\n",
        "for _ in range(1000):\n",
        "  y = (x - 1).pow(2) + 2\n",
        "  y.backward()\n",
        "\n",
        "  x.data = x.data - 0.01* x.grad.data\n",
        "  print(y.item(), x.item(), x.grad.data.item())\n",
        "  x.grad.data.zero_()\n",
        "\n",
        "print('y is minimum at x = 1')\n",
        "print('x = ', x.item())\n",
        "print('y = ', y.item())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "2.0 0.9999985694885254 -2.86102294921875e-06\n",
            "y is minimum at x = 1\n",
            "x =  0.9999985694885254\n",
            "y =  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzfnq7qbnH4t",
        "colab_type": "text"
      },
      "source": [
        "### Modules (layers)\n",
        "\n",
        "`Module` is a basic component of a computation graph in PyTorch, including all kinds of layers (https://pytorch.org/docs/stable/nn.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAp_UpbMnHcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "615ca2fc-2815-4151-ed9b-b869679e4f2e"
      },
      "source": [
        "# Layers\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "input_dim = 20\n",
        "output_dim = 30\n",
        "\n",
        "torch.manual_seed(123)\n",
        "input = torch.randn(10, input_dim)\n",
        "\n",
        "# Linear transform (fully connected layer)\n",
        "linear_module = nn.Linear(input_dim, output_dim, bias = True)\n",
        "output = linear_module(input)\n",
        "\n",
        "print('After linear', output.shape)\n",
        "\n",
        "# ReLU\n",
        "relu_module = nn.ReLU()\n",
        "relu_output = relu_module\n",
        "\n",
        "\n",
        "print('Before ReLU:', output[0, :5])\n",
        "print('Chaining modules:', relu_output[0, :5])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After linear torch.Size([10, 30])\n",
            "Before ReLU: tensor([ 0.3038,  0.5615, -0.4189, -0.0026,  0.0013], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e7fbd816b8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Before ReLU:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chaining modules:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'ReLU' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W71Xl5MOj73H",
        "colab_type": "text"
      },
      "source": [
        "### Sequential (sequence of modules)\n",
        "\n",
        "`nn.Sequential(module1, module2, ...)` can chain a sequence of modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syBPOxYkGLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d6ba9080-ae8f-452c-8c64-20a36b986490"
      },
      "source": [
        "# Combine modules\n",
        "sequential_module = nn.Sequential(linear_module, relu_module)\n",
        "\n",
        "sequential_output = sequential_module(input)\n",
        "\n",
        "print('Sequential:', sequential_output[0, :5])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential: tensor([0.3038, 0.5615, 0.0000, 0.0000, 0.0013], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWq2sCIpLJY",
        "colab_type": "text"
      },
      "source": [
        "### Models\n",
        "\n",
        "To build complex modules and reuse them, you can define your own Module class, like `Model1`, `Model2`, and `Model3`.\n",
        "Modules (layers) defined in a module will be automatically added to the computation graph.\n",
        "\n",
        "Your custom module should inherit from `nn.Module`. `super().__init__()` should be added in your `__init__()` method.\n",
        "You also need to implement your own `forward()` function which describes the forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuYvIGJgDH7r",
        "colab_type": "text"
      },
      "source": [
        "Define layers using member variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cory4c0ypO5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "90407489-eb91-4a00-fc9c-0e93e14a74f6"
      },
      "source": [
        "class Model1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() # must be added before define you module\n",
        "    \n",
        "    # Define layers as member variables\n",
        "    self.linear_module = nn.Linear(input_dim, output_dim, bias = True)\n",
        "    self.relu = nn. ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, input):\n",
        "    # Define forward pass (input -> linear module -> relu -> output)\n",
        "    output = self.linear_module(input)\n",
        "    output = self.relu(output)\n",
        "    return output\n",
        "\n",
        "  \n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Make a model\n",
        "model1 = Model1()\n",
        "\n",
        "# Run forward pass. model1(input) == model1.forward(input)\n",
        "model1_output = model1(input)\n",
        "\n",
        "print(model1)\n",
        "print('Model 1 output:', model1_output[0, :5])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model1(\n",
            "  (linear_module): Linear(in_features=20, out_features=30, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Model 1 output: tensor([0.0000, 0.4561, 0.0000, 0.0000, 0.0000], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiwedfslDPpk",
        "colab_type": "text"
      },
      "source": [
        "Define layers using `nn.Sequential`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8321V5bjWM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "5946d42e-a435-45ca-99a6-45e8ae5bc391"
      },
      "source": [
        "class Model2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Define layers as a sequential\n",
        "    \n",
        "    linear = nn.Linear(input_dim, output_dim, bias=True)\n",
        "    relu = nn.ReLU()\n",
        "    self.seq= nn.Sequential(linear, relu)\n",
        "        \n",
        "    \n",
        "  def forward(self, input):\n",
        "    output = self.seq(input)   \n",
        "    return output \n",
        "  \n",
        "  \n",
        "torch.manual_seed(123)\n",
        "model2 = Model2()\n",
        "model2_output = model2(input)\n",
        "\n",
        "print(model2)\n",
        "print('Model 2 output:', model2_output[0, :5])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model2(\n",
            "  (seq): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n",
            "Model 2 output: tensor([0.0000, 0.4561, 0.0000, 0.0000, 0.0000], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diqlp6CUDTF0",
        "colab_type": "text"
      },
      "source": [
        "Define layers using `nn.ModuleList`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r77JGC1jZKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "1087d4b3-d8cb-4e7a-a7e5-ea5ce821479a"
      },
      "source": [
        "class Model3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Define layers as a module list\n",
        "    pass\n",
        "    linear = nn.Linear(input_dim, output_dim, bias = True)\n",
        "    relu = nn. ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.module_list = nn.ModuleList([linear, relu])\n",
        "    print(input_dim)\n",
        "    print(output_dim)\n",
        "\n",
        "#     self.list = [linear, relu]\n",
        "    \n",
        "  \n",
        "#       self.seq= nn.Sequential(linear, relu)\n",
        "\n",
        "    \n",
        "  def forward(self, input):\n",
        "    for i in range(10):\n",
        "      output1 = self.module_list[0](input)\n",
        "      output2 = self.module_list[1](input)\n",
        "      return output2\n",
        "\n",
        "  \n",
        "torch.manual_seed(123)\n",
        "model3 = Model3()\n",
        "model3_output = model3(input)\n",
        "\n",
        "print(model3)\n",
        "print('Model 3 output:', model3_output[0, :5])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "30\n",
            "Model3(\n",
            "  (sigmoid): Sigmoid()\n",
            "  (module_list): ModuleList(\n",
            "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n",
            "Model 3 output: tensor([0.3374, 0.0000, 0.0000, 0.0000, 0.3486])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYTp4d6lh2h",
        "colab_type": "text"
      },
      "source": [
        "### Simple MLP module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9hI4u88lbV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "077da0f8-200b-4dc1-981a-949e142221c7"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dims=[]):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Define layers\n",
        "    fc = []\n",
        "    previous_dim = input_dim\n",
        "    for dim in hidden_dims:\n",
        "      fc.append(\n",
        "          nn.Linear(previous_dim, dim)\n",
        "      )\n",
        "      fc.append(\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      previous_dim = dim\n",
        "    \n",
        "    fc.append(\n",
        "        nn.Linear(previous_dim, output_dim)\n",
        "    )\n",
        "    \n",
        "    # Convert a list of layers to a sequential module\n",
        "    self.fc = nn.Sequential(*fc)\n",
        "  \n",
        "  def forward(self, observation):\n",
        "    return self.fc(observation)\n",
        "  \n",
        "  \n",
        "mlp = MLP(100, 20, [30, 30, 40, 50])\n",
        "print(mlp)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=30, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=30, out_features=40, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=40, out_features=50, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=50, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFxguuPOlkso",
        "colab_type": "text"
      },
      "source": [
        "### Be careful!\n",
        "\n",
        "If layers are not referenced by member variables and not added to `Sequential`, `ModuleList`, and `ModuleDict`, your module cannot update those layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POEGEaoZlnT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrong MLP module\n",
        "class MLP_wrong(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dims=[]):\n",
        "    super().__init__()\n",
        "    \n",
        "    fc = []\n",
        "    previous_dim = input_dim\n",
        "    for dim in hidden_dims:\n",
        "      fc.append(\n",
        "          nn.Linear(previous_dim, dim)\n",
        "      )\n",
        "      fc.append(\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      previous_dim = dim\n",
        "    \n",
        "    fc.append(\n",
        "        nn.Linear(previous_dim, output_dim)\n",
        "    )\n",
        "    \n",
        "    self.fc = fc # MLP module does not know about these layers\n",
        "    \n",
        "  def forward(self, observation):\n",
        "    output = observation\n",
        "    for layer in self.fc:\n",
        "      output = self.fc(output)\n",
        "    return output\n",
        "\n",
        "  \n",
        "mlp_wrong = MLP_wrong(100, 20, [30, 30])\n",
        "print(mlp_wrong)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0T_QNdipHq9",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ZtNkQKhQiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizers\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "true_output = torch.rand(output_dim)\n",
        "rnd_input = torch.rand(input_dim)\n",
        "\n",
        "for _ in range(10):\n",
        "  # Predicted output\n",
        "  predicted_output = model2(rnd_input)\n",
        "  \n",
        "  # Compute loss\n",
        "  loss = loss_fn(predicted_output, true_output)\n",
        "  print(loss)\n",
        "\n",
        "  # Update model\n",
        "  optimizer.zero_grad() # Zero out previous gradients\n",
        "  loss.backward()  # Compute new gradients\n",
        "  optimizer.step()  # Update parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhZ3sZgiPUI",
        "colab_type": "text"
      },
      "source": [
        "### GPU support\n",
        "\n",
        "You need to define your tensors in GPU.\n",
        "\n",
        "It can be done by \n",
        "\n",
        "`x = torch.tensor(data, device=torch.device(\"cuda\"))` \n",
        "\n",
        "or \n",
        "\n",
        "`x = torch.tensor(data).to(torch.device(\"cuda\"))`.\n",
        "\n",
        "or \n",
        "\n",
        "`x = torch.tensor(data).cuda()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnclDosiR9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cpu\") \n",
        "# For gpu\n",
        "# device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDV-ghZJRm4T",
        "colab_type": "text"
      },
      "source": [
        "## OpenAI gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFLNQXMWP0mf",
        "colab_type": "text"
      },
      "source": [
        "### Installation\n",
        "\n",
        "You can simply install OpenAI gym by executing `pip install gym`.\n",
        "\n",
        "Here, we are using a virtual display to render videos in a headless server. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnEOmhbeuL42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df192876-f491-407b-a9ed-bf082fc389c3"
      },
      "source": [
        "# Render OpenAI gym: code from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t\n",
        "\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y > /dev/null 2>&1\n",
        "!apt install xvfb -y > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pyglet==1.3.2 > /dev/null 2>&1\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TInxwg9Wyex6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from collections import defaultdict\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cx1kmBfQVbo",
        "colab_type": "text"
      },
      "source": [
        "### Make an environment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtV4hhHdy7Bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "dd88492e-58a4-4752-e0b0-91ed0cce1051"
      },
      "source": [
        "# Make an environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Observation space\n",
        "print('Observation space:', env.observation_space)\n",
        "\n",
        "# Action space\n",
        "print('Action space:', env.action_space)\n",
        "\n",
        "# Reset an episode and get an initial state\n",
        "observation = env.reset()\n",
        "\n",
        "print('Initial observation:', observation)\n",
        "\n",
        "# Render an image\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(4,)\n",
            "Action space: Discrete(2)\n",
            "Initial observation: [ 0.03  0.02 -0.01  0.04]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa96eef72e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEBCAYAAACUmXXrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwCJEHwAs8n9r7L3SKjbwnwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXqAaAx3pyyx",
        "colab_type": "text"
      },
      "source": [
        "### Take a step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR2CQcCNQ1Fc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "f6eb7bc5-894b-401b-f232-a72fe2d550d3"
      },
      "source": [
        "# Sample one random action\n",
        "random_action = env.action_space.sample()\n",
        "print('Random action', random_action)\n",
        "\n",
        "# Take a step\n",
        "observation_next, reward, done, info = env.step(random_action)\n",
        "\n",
        "print('Observation', observation)\n",
        "print('Next observation', observation_next)\n",
        "print('Reward', reward)\n",
        "print('Done', done)\n",
        "print('Info', info)\n",
        "\n",
        "# Render an image\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random action 0\n",
            "Observation [ 0.03  0.02 -0.01  0.04]\n",
            "Next observation [ 0.03 -0.18 -0.01  0.33]\n",
            "Reward 1.0\n",
            "Done False\n",
            "Info {}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa96c6116d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEBCAYAAACUmXXrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFRFJREFUeJzt3X9M1Pfhx/EXhzmcXeE8DHpgo9EU\nc5NkbneJf7llpxksQ6VZNshFt6S1NiElToPTTYUGNQw0ZltKg43NkiVE/mGKUAfaELPNZEbLWMZs\nrDHq2Ljp+DXxByzcvb9/9NtLzerdIVyP4/18JMRy789573fv+uTTz33uQ4YxxggAYAVHqicAAPji\nEH0AsAjRBwCLEH0AsAjRBwCLEH0AsAjRBwCLEH0AsEhSo3/79m2Vl5eruLhY5eXlunPnTjIfDgAQ\nR1KjX1tbq2AwqO7ubgWDQdXU1CTz4QAAcSQt+sPDw7p+/bpKS0slSaWlpbp+/bpGRkaS9ZAAgDiS\nFv1QKKSlS5cqMzNTkpSZmam8vDyFQqFkPSQAIA7eyAUAiyQt+h6PR/fu3VM4HJYkhcNh3b9/Xx6P\nJ1kPCQCII2nRz83NldfrVWdnpySps7NTXq9Xbrc7WQ8JAIgjI5nX079165b279+vBw8eKDs7Ww0N\nDVq1alWyHg4AEEdSow8AmFt4IxcALEL0AcAiRB8ALEL0AcAiRB8ALEL0AcAiRB8ALEL0AcAiRB8A\nLEL0AcAiRB8ALEL0AcAiRB8ALEL0AcAiRB8ALEL0AcAiRB8ALEL0AcAiRB8ALLJgpn9BIBCQ0+lU\nVlaWJKm6ulobNmxQX1+fampqNDk5qYKCAh07dky5ubkznjAA4PnN+BejBwIBNTc3q7CwMHpbJBJR\ncXGx6uvr5ff79c4772hgYED19fUznjAA4Pkl5fBOf3+/srKy5Pf7JUkVFRXq6upKxkMBAKZhxod3\npE8O6Rhj5PP5tGfPHoVCIeXn50fH3W63IpGIxsbG5HK5ZuMhAQDPYcZ7+i0tLTp37pza2tpkjFFd\nXd1szAsAkAQzjr7H45EkOZ1OBYNB9fb2yuPxaHBwMLrNyMiIHA4He/kAkGIziv7jx481Pj4uSTLG\n6Pz58/J6vSoqKtLExISuXbsmSWptbVVJScnMZwsAmJEZnb0zMDCgqqoqhcNhRSIRrV69WgcPHlRe\nXp56e3tVW1v71CmbS5Ysmc25AwCmacanbAIA0gefyAUAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI\n0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALBI3Og3\nNDQoEAhozZo1+vjjj6O33759W+Xl5SouLlZ5ebnu3LmT0BgAIHXiRn/jxo1qaWlRQUHBU7fX1tYq\nGAyqu7tbwWBQNTU1CY0BAFInbvT9fr88Hs9Ttw0PD+v69esqLS2VJJWWlur69esaGRmJOQYASK0F\nz3OnUCikpUuXKjMzU5KUmZmpvLw8hUIhGWOeOeZ2u2dv5gCAaeONXACwyHPt6Xs8Ht27d0/hcFiZ\nmZkKh8O6f/++PB6PjDHPHAMApNZz7enn5ubK6/Wqs7NTktTZ2Smv1yu32x1zDACQWhnGGBNrgyNH\njujChQsaGhrS4sWL5XK59P777+vWrVvav3+/Hjx4oOzsbDU0NGjVqlWSFHMMAJA6caMPAJg/eCMX\nACxC9AHAIkQfACxC9AHAIkQfACxC9AHAIkQfACxC9AHAIkQfACxC9AHAIkQfACxC9AHAIkQfACxC\n9AHAIkQfACxC9AHAIkQfACxC9AHAIgsS2aihoUHd3d365z//qY6ODhUWFkqSAoGAnE6nsrKyJEnV\n1dXasGGDJKmvr081NTWanJxUQUGBjh07ptzc3CQtAwCQiIT29Ddu3KiWlhYVFBT8z9ivfvUrtbe3\nq729PRr8SCSivXv3qqamRt3d3fL7/Tp+/PjszhwAMG0JRd/v98vj8ST8l/b39ysrK0t+v1+SVFFR\noa6uruebIQBg1iR0eCeW6upqGWPk8/m0Z88eZWdnKxQKKT8/P7qN2+1WJBLR2NiYXC7XTB8SAPCc\nZvRGbktLi86dO6e2tjYZY1RXVzdb8wIAJMGMov/pIR+n06lgMKje3t7o7YODg9HtRkZG5HA42MsH\ngBR77ug/fvxY4+PjkiRjjM6fPy+v1ytJKioq0sTEhK5duyZJam1tVUlJySxMFwAwExnGGBNvoyNH\njujChQsaGhrS4sWL5XK51NzcrKqqKoXDYUUiEa1evVoHDx5UXl6eJKm3t1e1tbVPnbK5ZMmSpC8I\nAPBsCUUfADA/8IlcALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9IIYP330j+uenX0A6I/oAYBGi\nDwAWIfoAYBGiDwAWIfoAYBGiDwAWIfoAYBGiDwAWIfoAYBGiDwAWIfoAYJG40R8dHdXrr7+u4uJi\nbd68WW+++aZGRkYkSX19fdqyZYuKi4v16quvanh4OHq/WGMAgNSIG/2MjAzt2LFD3d3d6ujo0Esv\nvaTjx48rEolo7969qqmpUXd3t/x+v44fPy5JMccAAKkTN/oul0vr16+Pfr9u3ToNDg6qv79fWVlZ\n8vv9kqSKigp1dXVJUswxAEDqTOuYfiQS0enTpxUIBBQKhZSfnx8dc7vdikQiGhsbizkGAEidBdPZ\n+PDhw1q0aJG2bdumixcvJmtOwJzh23nyqT+BdJdw9BsaGnT37l01NzfL4XDI4/FocHAwOj4yMiKH\nwyGXyxVzDEgnH777hnw7T/7PL0/hhwDSVUKHd06cOKH+/n41NTXJ6XRKkoqKijQxMaFr165Jklpb\nW1VSUhJ3DACQOnH39G/evKmTJ09q5cqVqqiokCQtX75cTU1NamxsVG1trSYnJ1VQUKBjx45JkhwO\nxzPHAACpEzf6L7/8sm7cuPG5Y1//+tfV0dEx7TEAQGrwiVwAsAjRBwCLEH0AsAjRBwCLEH0AsAjR\nBwCLEH0AsAjRBwCLEH0AsAjRBwCLEH0AsAjRBwCLEH0AsAjRBwCLEH0AsAjRBwCLEH0AsAjRBwCL\nxP11iaOjo/rJT36iv//973I6nVqxYoXq6urkdru1Zs0aFRYWyuH45GdHY2Oj1qxZI0nq6elRY2Oj\nwuGw1q5dq/r6en3pS19K7moAADHF3dPPyMjQjh071N3drY6ODr300ks6fvx4dLy1tVXt7e1qb2+P\nBv/Ro0c6dOiQmpubdfHiRb3wwgt67733krcKAEBC4kbf5XJp/fr10e/XrVunwcHBmPf5/e9/r6Ki\nIq1cuVKSVFFRod/97nczmykAYMbiHt75rEgkotOnTysQCERv2759u8LhsL7xjW+oqqpKTqdToVBI\n+fn50W3y8/MVCoVmb9YAgOcyregfPnxYixYt0rZt2yRJly5dksfj0cOHD7V37141NTVp9+7dSZko\nkAq+nSef+hNIdwlHv6GhQXfv3lVzc3P0jVuPxyNJ+vKXv6zvf//7+vWvfx29/cqVK9H7Dg4ORrcF\n0smH774h386T+vDdN566nR8CSFcJnbJ54sQJ9ff3q6mpSU6nU5L0n//8RxMTE5KkqakpdXd3y+v1\nSpI2bNigv/71r7pz546kT97s/c53vpOE6QMApiPunv7Nmzd18uRJrVy5UhUVFZKk5cuXa8eOHaqp\nqVFGRoampqb0ta99Tbt27ZL0yZ5/XV2d3njjDUUiEXm9Xh04cCC5KwEAxBU3+i+//LJu3LjxuWMd\nHR3PvN+mTZu0adOm558ZAGDW8YlcALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9\nALAI0QcAixB9ALAI0QcAixB9ALAI0YeVMjIyEvqa6f1j/R1AKhB9ALDItH5HLmCrztDO6D+Xet5N\n4UyAmWFPH5imz/4AANIN0QdiIPCYb4g+EMNbb/lTPQVgViUU/crKSm3ZskVlZWUKBoP66KOPJEm3\nb99WeXm5iouLVV5erjt37kTvE2sMSGcc00c6yzDGmHgbjY+P68UXX5QkffDBB2pqatKZM2f0wx/+\nUN/73ve0detWtbe3q62tTb/5zW8kKeYYkGrTOZXSGDOjUy8T+E8M+OKYaTpz5ox55ZVXzNDQkPH5\nfGZqasoYY8zU1JTx+XxmeHg45th0rFixwkiaV1///0N2Xn7N17WxrvT7mo9rW7FixXRz/bkSPmXz\nwIEDunz5sowxOnXqlEKhkJYuXarMzExJUmZmpvLy8hQKhWSMeeaY2+1O9CHn7SEhM4/3/Obr2lhX\n+pnPa5uJhKN/9OhRSdLZs2fV2NioXbt2JW1SQLJxeAe2mvbZO2VlZbpy5YqWLVume/fuKRwOS5LC\n4bDu378vj8cjj8fzzDEAQOrEjf6jR48UCoWi3/f09CgnJ0e5ubnyer3q7OyUJHV2dsrr9crtdscc\nAwCkTtyzd4aGhlRZWaknT57I4XAoJydH+/bt09q1a3Xr1i3t379fDx48UHZ2thoaGrRq1SpJijkG\npBqHd2CrhE7ZBOYbog9b8YlcALAI0QcAi3B4BwAswp4+AFiE6AOARYg+AFiE6AOARYg+AFiE6AOA\nRYg+AFiE6AOARYg+AFiE6AOARYg+AFiE6AOARYg+AFiE6AOARYg+AFhkQSIbVVZW6h//+IccDocW\nLVqkQ4cOyev1KhAIyOl0KisrS5JUXV2tDRs2SJL6+vpUU1OjyclJFRQU6NixY8rNzU3eSgAAcSX0\nS1TGx8f14osvSpI++OADNTU16cyZMwoEAmpublZhYeFT20ciERUXF6u+vl5+v1/vvPOOBgYGVF9f\nn5xVAAASktDhnU+DL0kPHz6M+0ui+/v7lZWVJb/fL0mqqKhQV1fXDKYJAJgNCR3ekaQDBw7o8uXL\nMsbo1KlT0durq6tljJHP59OePXuUnZ2tUCik/Pz86DZut1uRSERjY2NyuVyzuwIAQMISfiP36NGj\nunTpknbv3q3GxkZJUktLi86dO6e2tjYZY1RXV5e0iQIAZm7aZ++UlZXpypUrGh0dlcfjkSQ5nU4F\ng0H19vZKkjwejwYHB6P3GRkZkcPhYC8fAFIsbvQfPXqkUCgU/b6np0c5OTnKysrS+Pi4JMkYo/Pn\nz8vr9UqSioqKNDExoWvXrkmSWltbVVJSkoz5AwCmIe7ZO0NDQ6qsrNSTJ0/kcDiUk5Ojffv2KTs7\nW1VVVQqHw4pEIlq9erUOHjyovLw8SVJvb69qa2ufOmVzyZIlX8iiAACfL6FTNgEA8wOfyAUAixB9\nALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI0QcAixB9ALAI\n0QcAixB9ALAI0QcAixB9ALDItKL/9ttva82aNfr4448lSX19fdqyZYuKi4v16quvanh4OLptrDEA\nQGokHP2//e1v6uvrU0FBgSQpEolo7969qqmpUXd3t/x+v44fPx53DACQOglF/7///a/q6ur01ltv\nRW/r7+9XVlaW/H6/JKmiokJdXV1xxwAAqZNQ9H/5y19qy5YtWr58efS2UCik/Pz86Pdut1uRSERj\nY2MxxwAAqRM3+n/+85/V39+vYDD4RcwHAJBEC+JtcPXqVd26dUsbN26UJP3rX//Sa6+9pu3bt2tw\ncDC63cjIiBwOh1wulzwezzPHAACpE3dPf+fOnfrjH/+onp4e9fT0aNmyZXrvvfe0Y8cOTUxM6Nq1\na5Kk1tZWlZSUSJKKioqeOQYASJ24e/rP4nA41NjYqNraWk1OTqqgoEDHjh2LOwYASJ0MY4xJ9SQA\nAF8MPpELABYh+gBgEaIPABYh+gBgkTkX/du3b6u8vFzFxcUqLy/XnTt3Uj2lhDU0NCgQCDx1UTop\n9prSYb2jo6N6/fXXVVxcrM2bN+vNN9/UyMiIpPS/6F5lZaW2bNmisrIyBYNBffTRR5LS/zn71Hy8\nSGIgEFBJSYm2bt2qrVu36g9/+IOk9F/b5OSkamtr9e1vf1ubN2/WoUOHJCXhtWjmmO3bt5uzZ88a\nY4w5e/as2b59e4pnlLirV6+awcFB861vfcvcuHEjenusNaXDekdHR82f/vSn6Pc///nPzU9/+lMT\nDofNpk2bzNWrV40xxjQ1NZn9+/cbY0zMsbnkwYMH0X++ePGiKSsrM8ak/3NmjDH9/f3mtddei74e\n58PzZYz5n/++jIk9/3RZ2+HDh83Ro0dNJBIxxhjz73//2xgz+6/FORX9oaEh4/P5zNTUlDHGmKmp\nKePz+czw8HCKZzY9n31RxlpTuq63q6vL/OhHPzJ/+ctfzHe/+93o7cPDw2bdunXGGBNzbK46c+aM\neeWVV+bFczY5OWl+8IMfmIGBgejrcb48X58X/XRf28OHD43P5zMPHz586vZkvBaf+8NZyRAKhbR0\n6VJlZmZKkjIzM5WXl6dQKCS3253i2T2fWGsyxqTdeiORiE6fPq1AIPDcF92ba5fjOHDggC5fvixj\njE6dOjUvnrPZvEjiXHu+JKm6ulrGGPl8Pu3Zsyft1zYwMCCXy6W3335bV65c0QsvvKBdu3Zp4cKF\ns/5anHPH9DG3HT58WIsWLdK2bdtSPZVZc/ToUV26dEm7d+9WY2NjqqczY/P9IoktLS06d+6c2tra\nZIxRXV1dqqc0Y+FwWAMDA/rKV76i3/72t6qurlZVVZUeP3486481p6Lv8Xh07949hcNhSZ/8i7h/\n/748Hk+KZ/b8Yq0p3dbb0NCgu3fv6he/+IUcDkfMC+ul40X3ysrKdOXKFS1btiytn7PPXiQxEAhE\nL5J49+7defF8ffrv2ul0KhgMqre3N+1fix6PRwsWLFBpaakk6atf/aoWL16shQsXzvprcU5FPzc3\nV16vV52dnZKkzs5Oeb3eOfW/zdMVa03ptN4TJ06ov79fTU1NcjqdkmJfWC8dLrr36NEjhUKh6Pc9\nPT3KyclJ++dsPl8k8fHjxxofH5ckGWN0/vx5eb3etH8tut1urV+/XpcvX5b0yVk5w8PDWrly5ay/\nFufctXdu3bql/fv368GDB8rOzlZDQ4NWrVqV6mkl5MiRI7pw4YKGhoa0ePFiuVwuvf/++zHXlA7r\nvXnzpkpLS7Vy5UotXLhQkrR8+XI1NTWpt7f3fy6st2TJEkmKOTYXDA0NqbKyUk+ePJHD4VBOTo72\n7duntWvXpv1z9lmBQEDNzc0qLCxM6+dL+uTYd1VVlcLhsCKRiFavXq2DBw8qLy9vXqztZz/7mcbG\nxrRgwQL9+Mc/1je/+c1Zfy3OuegDAJJnTh3eAQAkF9EHAIsQfQCwCNEHAIsQfQCwCNEHAIsQfQCw\nCNEHAIv8H0M2vDkXyLe5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrrmm_yAqF-4",
        "colab_type": "text"
      },
      "source": [
        "### Run one episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqAoppv-RGL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "68868e00-04bd-428b-aed6-e84d52eb0655"
      },
      "source": [
        "# Make a new environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Wrap environment for recording\n",
        "env = wrap_env(env)\n",
        "\n",
        "# Initialize environment\n",
        "observation = env.reset()\n",
        "\n",
        "done = False\n",
        "episode_reward = 0\n",
        "episode_length = 0\n",
        "\n",
        "# Run until done == True\n",
        "while not done:\n",
        "  # Take a step\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "  \n",
        "  episode_reward += reward\n",
        "  episode_length += 1\n",
        "  \n",
        "print('Total reward:', episode_reward)\n",
        "print('Total length:', episode_length)\n",
        "\n",
        "env.close()\n",
        "show_video()  "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 12.0\n",
            "Total length: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAChRtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAB8GWIhAAv//72rvzLK0cLlS4dWXuzUfLoSXL9iDB9aAAAAwAAAwAAJuKiZ0WFMeJsgAAALmAIWElDyDzETFWKgSvGXwOeIAKPT+PmAGPWHNsgSW0mV7HNuyp0IdwhsNfp8f7LkvOQ0WalP31gEyunSDx+2LWWCZxvnrjwU8zLOtCXqfzxbxlj5F2p3ZUW9wQ5Uk0wOdsGddG75Z/zGaiuZrMC9j/uNVlwuAwlq0k2GXnNaMbkJmz4qfs+LU+V+2varIi9DwakAg3qwWD4wIlEu7y/oH9pRQb7eDTdq8Yzf0GfxDVVnXvJAJAuJT5olirj8zIa55M+EXLWKtRSgzOLBNKQfn/Kjtd8XBTSgAA6LmaEZeaAK/jcaFNryPxrm5yODgAGznDPMtEY6OD3m8LEYEevYKIc1lZDLYn0qjytQkInKp4SUun0ca2W5s7hC4DxaSbP/7rzRTeRNpBFgZGa8sEc4kZeLPRrYtH7jqmmzSgFpZmvR9VRKl5OsG+n2fW7pBjqtHpsmDPaUEmFJ8+R3fQ7xJ93SoyVkbe8b+G0SKiLwCpkv+2yK3/NooazK8FNNWAAAmF0VPeqAusFRtfYgvQdv1Uqv8hwOVtI1RNEqy41g4Iry18uFKi4vO3E27XZ3RVMH2w8nAAAAwAAAwAABAUAAAB8QZokbEK//jhAAAEFzkh8ysT4UClkAHNxYYp/brgeytRwCZQ42hO2lboWkaE5RF5nJI3XRzs4BJ/ZTb/+g+dRg7H2z/4pQv+tYuxrZSHOPOodBJinxkVATni6imKOmaOVIaVhL5g/lcAAAZq2CtXys3fTxVsHf0s6oTjHwAAAAFBBnkJ4hH8AABYuYXDDPukjlhQGNm9hWN5fzyGF4nllxjMce3KrUnNfocH/EhRiTgCOdsHz04XLJofaNzytihlNW3U3AAADAACO3usBiqAfkQAAAEoBnmF0R/8AAA2F3TMpBEPYXhI1eeFZbUcWLIhK/o9T1ao5emkGACZhqAjjG5WRrVewQ1AfOD4Wr6aZ5AAAAwAAAwAs6DTMusA9IAAAADkBnmNqR/8AACK/H42XNE8r5so6NktIRoNAS7O8fybw5sZEertEbZ50ilDAAAADALjS74iWezjwgScAAABOQZplSahBaJlMCFf//jhAAAEM+iBlMFSD7OKqAGs7By9YAmbOtk0pPu3qlgze0lAdyHaTgYrf0gOe9ea1nOOmMrCJxx5FqKe1fYhbup3TAAAAwUGah0nhClJlMFESwr/+OEAAAQ1PLhEyAsswAtaiggSM5XdKV6eb+1rIVirCiaiDm33xt5DHZwAtpXa/1CkxFkMJIAG6pEVGMEcDb9QqUwVULlOrWfXFKqdP3PyWuTWTcyR3wDv6C1FEL5+qNERjMumaWWtC70Y6OA+3qdIzjjcF+QuJe3sYVTuIWZqCdRoIs7f9IrZJOerelndAopB7R2SefgjW3jBnkKlOV08Pj+gz989a1twAbKj42oTmAJIhvSUAAAA6AZ6makf/AAAjwj8k1IANwVV3QbyVt7f2jJZejTgUFcspC23IvmnGanlP/jEC0wOa4zT7KaY6gKTFgQAAAOdBmqtJ4Q6JlMCEf/3hAAAEFowSfI6up1vOrLAOxkADin5GBz51f7y9Z3fbJJNeD264VvmHoJDedjK83bNBOXmLNI/olsD0FHCi8wO4h6yLjCC9GMeIB7h0///ayKUJhKcHBP/+dUckpvnAEL0q66rKLbPMI5bhXS2lrUhHIhoB3s1y371kBy8QncENy6AyLjqlEIZLbyR95oROWPK/CADCNhrmnAZPMmg80Fiwqt98+B/6U+gcoHj6eltbiGCn+EyXk54WPLNS9NKyDb7GdvBERr8iE9PEelaKA7aU5o8sDEdc2MH5cUwAAABzQZ7JRRU8I/8AABa7Y5g66IhFLUexLWOxbAAuouMgbjwWkb4ZZ3b2y+YCbHB990nD+e7++fzP36bfGl2rvtoNX5SHsZolCuW70+trnXXg31GhEguwsqyf1fzyD3z7n3Kdkg9hwSxMfm7DhO9rH3qFP4NqVgAAAF8Bnuh0R/8AACPAmSl+WAEZAIUERsHMO76H+IAn6ONntJ8VD8aFxYZxw8AdRz/Wv3K0hURfLAPyj/OW2WScpKeL4IYRfIotDlX8Y+JzLsDIRRigOi79iFqiqCTxxkVKwQAAAGMBnupqR/8AACNDvYf3o7UC0HKK3VSaEFzQJAyKo4Sy4dSgnFoIlj/CFsAFz+7fp+y2dZqbU5M8X4PDPzRsTUhuFtVhldtGh2+2ksMx79nmE2G1DEkTuHHEVzPvD08723f+jegAAACCQZrsSahBaJlMCP/8hAAAD9Oj2MPZ3r0kfoALa7ZaMhs2UkMKmFJO3uJx6PVzfQrmg80YP1oVJH149bRjMcJt1yqBkDzhYLdbkOkHxc0m7F/5mPxvzSXoJp/tYFKaUMzKvog0TJQBALBo/PrwkMCHfA89ZlhxD8X4I3XUQmLIVnv4wAAAA69tb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAABBAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAC2XRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAABBAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACWAAAAZAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAAQQAAAIAAAEAAAAAAlFtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADIAAAANAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAH8bWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAABvHN0YmwAAACYc3RzZAAAAAAAAAABAAAAiGF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAACWAGQAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAyYXZjQwFkAB//4QAZZ2QAH6zZQJgz5eEAAAMAAQAAAwBkDxgxlgEABmjr48siwAAAABhzdHRzAAAAAAAAAAEAAAANAAABAAAAABRzdHNzAAAAAAAAAAEAAAABAAAAeGN0dHMAAAAAAAAADQAAAAEAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAACAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAIAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAAANAAAAAQAAAEhzdHN6AAAAAAAAAAAAAAANAAAEpgAAAIAAAABUAAAATgAAAD0AAABSAAAAxQAAAD4AAADrAAAAdwAAAGMAAABnAAAAhgAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1Ny44My4xMDA=\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkyoyKkQRr3S",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-VXdfNV_y5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset for Q-learning\n",
        "# Also called \"experience replay buffer\", \"replay memory\"\n",
        "class Dataset(object):\n",
        "  \n",
        "  def __init__(self, size):\n",
        "    self._size = size\n",
        "    self._transitions = []\n",
        "    self._index = 0\n",
        "    \n",
        "  # Store a transition (s_t, a_t, s_t+1, r_t, done)\n",
        "  def store(self, obs, action, obs_next, reward, done):\n",
        "    transition = {'obs': obs, 'action': action, \n",
        "                  'obs_next': obs_next, 'reward': reward, 'done': done}\n",
        "    \n",
        "    if self._index < self._size:\n",
        "      self._transitions.append(transition)\n",
        "    else:\n",
        "      self._transitions[self._index] = transition\n",
        "      \n",
        "    self._index = (self._index + 1) % self._size\n",
        "\n",
        "  # Sample `batch_size` transitions from the dataset and convert to tensors\n",
        "  def sample(self, batch_size):\n",
        "    indexes = np.random.randint(0, len(self._transitions), batch_size)\n",
        "    batch = {\n",
        "        k: np.stack([self._transitions[i][k] for i in indexes])\n",
        "        for k in ['obs', 'action', 'obs_next', 'reward', 'done']}\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z537CR_mmW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Logger(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self._history = defaultdict(list)\n",
        "    self._global_history = defaultdict(list)\n",
        "  \n",
        "  def add(self, info):\n",
        "    for k, v in info.items():\n",
        "      self._history[k].append(v)\n",
        "      self._global_history[k].append(v)\n",
        "      \n",
        "  def summary(self):\n",
        "    summary = []\n",
        "    for k, v in self._history.items():\n",
        "      summary.append('{}: {:.2f}'.format(k, np.mean(v)))\n",
        "      \n",
        "    print(', '.join(summary))\n",
        "      \n",
        "    self._history = defaultdict(list)\n",
        "    \n",
        "  def plot(self, key):\n",
        "    n = len(self._global_history[key])\n",
        "    plt.plot(np.arange(n), self._global_history[key])\n",
        "    plt.title(key)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzOw9o0u_zqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q-network, an approximation of a Q-value function, Q(s, a)\n",
        "class QNetwork(nn.Module):\n",
        "  \n",
        "  def __init__(self, observation_space, action_space):\n",
        "    super().__init__()\n",
        "\n",
        "    self._action_space = action_space\n",
        "    self._eps = 1\n",
        "\n",
        "    # Input: observation\n",
        "    # Output: q-value for each action\n",
        "    self.fc = MLP(observation_space.shape[0], action_space.n, [32, 32])\n",
        "    print(self.fc)\n",
        "    \n",
        "  def forward(self, observation):\n",
        "    q_values = self.fc(observation)\n",
        "    return q_values\n",
        "  \n",
        "  def act(self, observation, is_train=True):\n",
        "    # Epsilon greedy for exploration\n",
        "    if is_train and np.random.uniform() < self._eps:\n",
        "      action = self._action_space.sample()\n",
        "    else:\n",
        "      # Observation is numpy ndarray\n",
        "      # Convert numpy array to tensor\n",
        "      obs = torch.tensor([observation]).to(device, dtype=torch.float32)\n",
        "      \n",
        "      # Compute q-values\n",
        "      q_values = self.forward(obs)\n",
        "      q_values = q_values.detach().cpu().numpy().squeeze()\n",
        "      \n",
        "      # Take an action with maximum value\n",
        "      action = np.argmax(q_values)\n",
        "    \n",
        "    return action\n",
        "  \n",
        "  def epsilon_decay(self):\n",
        "    self._eps -= 0.01\n",
        "    self._eps = max(self._eps, 0.01)\n",
        "    \n",
        "  def state_dict(self):\n",
        "    state_dict = super().state_dict()\n",
        "    state_dict['eps'] = self._eps\n",
        "    return state_dict\n",
        "  \n",
        "  def load_state_dict(self, state_dict):\n",
        "    self._eps = state_dict['eps']\n",
        "    state_dict.pop('eps')\n",
        "    super().load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ijX2c6iOXME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "  def __init__(self, env, model, dataset):\n",
        "    self._env = env\n",
        "    self._model = model\n",
        "    self._dataset = dataset\n",
        "    self._logger = Logger()\n",
        "    \n",
        "    self._optim = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "  def run_one_episode(self, env, is_train=True):\n",
        "    model = self._model\n",
        "    dataset = self._dataset\n",
        "    \n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    episode_length = 0\n",
        "    episode_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "      \n",
        "      # Sample an action\n",
        "      action = model.act(observation, is_train)\n",
        "      \n",
        "      # Take a step\n",
        "      observation_next, reward, done, info = env.step(action)\n",
        "      \n",
        "      # Add a transition to the dataset\n",
        "      dataset.store(observation, action, observation_next, reward, done)\n",
        "      observation = observation_next\n",
        "      \n",
        "      episode_length += 1\n",
        "      episode_reward += reward\n",
        "      \n",
        "      # Train q-network\n",
        "      if is_train:\n",
        "        batch = dataset.sample(batch_size=20)\n",
        "        loss = self._update(batch)\n",
        "        model.epsilon_decay()\n",
        "        \n",
        "        self._logger.add({'loss': loss})\n",
        "        self._logger.add({'action': action})\n",
        "      \n",
        "    if is_train:\n",
        "      self._logger.add({'length': episode_length, 'reward': episode_reward})  \n",
        "    else:\n",
        "      print('reward: {}'.format(episode_reward))\n",
        "          \n",
        "  def _update(self, batch):\n",
        "    batch_size = len(batch['obs'])\n",
        "    \n",
        "    obs = torch.tensor(batch['obs'], dtype=torch.float32)\n",
        "    obs_next = torch.tensor(batch['obs_next'], dtype=torch.float32)\n",
        "    rew = torch.tensor(batch['reward'], dtype=torch.float32).reshape(batch_size, 1)\n",
        "    ac = torch.eye(self._env.action_space.n)[batch['action']]\n",
        "    done = torch.tensor(batch['done'], dtype=torch.float32).reshape(batch_size, 1)\n",
        "    \n",
        "    discount_factor = 0.95\n",
        "    \n",
        "    # y\n",
        "    with torch.no_grad():\n",
        "      q_next = self._model(obs_next).max(dim=1, keepdim=True)[0]\n",
        "      \n",
        "      q_target = rew + (1 - done) * discount_factor * q_next\n",
        "    \n",
        "    # Q(s,a)\n",
        "    q_predict = torch.sum(self._model(obs) * ac, dim=1, keepdim=True)    \n",
        "    \n",
        "    # Mean square error\n",
        "    loss = (q_predict - q_target).pow(2).mean()\n",
        "    \n",
        "    # Backpropagation\n",
        "    self._optim.zero_grad()\n",
        "    loss.backward()\n",
        "    self._optim.step()\n",
        "    \n",
        "    return loss.item()\n",
        "    \n",
        "  def train(self):\n",
        "    num_episode = 200\n",
        "    for i in range(num_episode):\n",
        "      self.run_one_episode(env=self._env, is_train=True)\n",
        "      \n",
        "      if (i + 1) % 10 == 0:\n",
        "        print('training episode {}/{}'.format(i + 1, num_episode))\n",
        "        self._logger.summary()\n",
        "\n",
        "  def evaluate(self):\n",
        "    env_test = wrap_env(gym.make('CartPole-v0'))\n",
        "    for i in range(10):\n",
        "      self.run_one_episode(env=env_test, is_train=False)\n",
        "    env_test.close()\n",
        "    show_video()\n",
        "    \n",
        "  def plot(self, key):\n",
        "    self._logger.plot(key)\n",
        "    \n",
        "  def state_dict(self):\n",
        "    return {'optim': self._optim.state_dict()}\n",
        "  \n",
        "  def load_state_dict(self, state_dict):\n",
        "    self._optim.load_state_dict(state_dict['optim'])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCR4K6sfM262",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64d33636-91fc-4d21-a2fd-3b21183af68a"
      },
      "source": [
        "# Make sure setting up random seeds before training\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa9d0a6ecd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URKNQPUjSaSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "06c3f689-5e49-4145-8907-ce711eb32ead"
      },
      "source": [
        "# Environment\n",
        "env = gym.make('CartPole-v0')\n",
        "env.seed(123)\n",
        "\n",
        "# Model\n",
        "policy = QNetwork(env.observation_space, env.action_space)\n",
        "print(policy)\n",
        "\n",
        "# Dataset\n",
        "dataset = Dataset(10000)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "QNetwork(\n",
            "  (fc): MLP(\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): ReLU()\n",
            "      (4): Linear(in_features=32, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZy2tyd0tkA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "a62a7fa3-56fd-4e57-9f92-6f9d2ca2ed5e"
      },
      "source": [
        "# Trainer\n",
        "trainer = Trainer(env, policy, dataset)\n",
        "\n",
        "# Train Q-Network\n",
        "policy.train()\n",
        "trainer.train()\n",
        "\n",
        "trainer.plot('reward')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-55e0d3477fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train Q-Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-4557166d19a3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, model, dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clm_5tH91QJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b2208c26-9003-4280-fbc9-0aedefb4f52f"
      },
      "source": [
        "# Save checkpoint\n",
        "os.makedirs('ckpt', exist_ok=True)\n",
        "ckpt_path = os.path.join('ckpt', 'ckpt.pt')\n",
        "\n",
        "# Get parameters from models and optimizers\n",
        "state_dict = {}\n",
        "state_dict['trainer'] = trainer.state_dict()\n",
        "state_dict['policy'] = policy.state_dict()\n",
        "\n",
        "torch.save(state_dict, ckpt_path)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-d7b4d1676ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get parameters from models and optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btv-MRUx1dmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "bb0bb4bf-a993-42e1-d01b-dfcb32fa9532"
      },
      "source": [
        "# Load checkpoint\n",
        "ckpt = torch.load(ckpt_path)\n",
        "\n",
        "# Update parameters with checkpoint\n",
        "trainer.load_state_dict(ckpt['trainer'])\n",
        "policy.load_state_dict(ckpt['policy'])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-9302dce0f784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Update parameters with checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ckpt/ckpt.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruzW9OIvJ6UO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "5958a3cc-7844-41b1-dd58-091f951dec56"
      },
      "source": [
        "# Run evaluation\n",
        "policy.eval()\n",
        "trainer.evaluate()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-f0aec0a71281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5dfibx0cnlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}